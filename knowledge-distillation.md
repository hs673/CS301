Knowledge Distillation is used to compress large, trained models into small models without significant loss in performance. In order to do this, we use the large model as a teacher to train the small model, the student. The methods builds off of the already trained large model by using the softmax of the large model with high temperature to produce a transfer set. The transfer set will then be used to train the distilled model using soft target distribution. 

The weighted average of the cross entropy of the distillation loss and the cross entropy of the student loss will allow us to find the total loss function to make our distilled model to produce the correct labels. The teacher model will produce soft labels while the student model will produce soft predictions. Distillation loss is calculated by the loss function between the soft labels and the soft predictions. In order to train the student to produce the correct labels, we change the temperature of the softmax of the distilled model to 1. This will produce hard predictions that we use to calculate the student loss.
