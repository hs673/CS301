Knowledge Distillation is used to compress large, trained models into small models without experiencing significant loss in performance. This is especially useful for distributing models for deployment because it takes less computational resources. The small model is trained to generalize like the large model which will perform better than training the small model with a normal data set. This generalization can be passed down by using class probabilties from the large model. Soft targets from the class probabilities allows the small model to benefit more from a smaller, well-rounded data set that will provide more information than training with hard targets.

In order to apply knowledge distillation on our model, we use our large model as a teacher to train the small model, the student. The methods builds off of the already trained large model by using the softmax of the large model with high temperature to produce a transfer set. The softmax is used to calculate class probability with the formula $q_{i} =\frac{exp(z_{i}/T )}{\sum_{j} exp(z_{j} /T)}$. $z_{i}$ is the logit which is compared with other logits to calculate $q_{i}$, the probability. Logits are used as source of our teacher knowledge to transfer to the student. The transfer set will then be used to train the distilled model using soft target distribution.

The weighted average of the cross entropy of the distillation loss and the cross entropy of the student loss will allow us to find the total loss function to make our distilled model produce the correct labels. The teacher model will produce soft labels while the student model will produce soft predictions. Distillation loss is calculated by the loss function between the soft labels and the soft predictions. In order to train the student to produce the correct labels, we use the same logits in the softmax of the distilled model and change the temperature to 1. This will produce hard predictions that we use to calculate the student loss.

We could not code the assigment because Keras is incompatible with Pytorch and the distillation code that was given to us was in Pytorch. 
